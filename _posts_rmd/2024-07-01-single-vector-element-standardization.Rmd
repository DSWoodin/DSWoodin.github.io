---
title: "Single Vector Element Standardization"
author: "Nathan Layman"
date: "`r Sys.Date()`"
output:
  md_document:
    variant: gfm
knit: >
  (function(input_file, encoding) {
    metadata <- rmarkdown::yaml_front_matter(input_file)
    output_file = with(metadata, paste0(Sys.Date(), "-", gsub(" ", "-", title), ".md"))
    output_dir = "../_posts"
    rmarkdown::render(input = input_file, output_file = output_file, output_dir = output_dir)
  })
---

`---`{=html} \
`layout: post `{=html} \
`title: Single Vector Element Standardization`{=html} \
`subtitle: Finding harmony within oneself`{=html} \
`image: /assets/img/inner-harmony.png`{=html} \
`cover-img: /assets/img/inner-harmony.png`{=html} \
`share-img: /assets/img/inner-harmony.png`{=html} \
`tags: [data, R, tricks]`{=html} \
`---`{=html} \


```{r setup, include = FALSE}
library(tidyverse)
```

Standardizing data vectors is an important step in data science. For example, different data sources often represent the same entity, such as country names, in different ways or with different spellings. In addition, small errors or formatting inconsistencies can complicate joins and lead to difficulties in downstream analysis. While excellent resources do exist to solve this problem for individual use cases, such as the [countries](https://github.com/fbellelli/countries) package in R, it can be useful to understand general methods to self-standardize categorical vectors. There are three approaches that I've used to standardize character vectors. I've written scripts below to illustrate each method.

## The vector to standardize
Note: this data is fictional!
```{r, echo = F}
outbreak_data <- tribble(
  ~disease_name,            ~outbreaks,
  "Influenza",              15,
  "Inflenza",               10,
  "COVID-19",               95,
  "sars-covid-19",          90,
  "Malaria",                20,
  "Maleria",                22,
  "malaria",                20,
  "Diabetes",               30,
  "Diabetis",               28,
  "HIV/AIDS",               75,
  "HIV",                    70,
  "AIDS",                   65,
  "Tuberculosis",           40,
  "Tuberclosis",            38,
  "Alzheimers",             23,
  "Alzheimers Disease",     27,
  "Heart Disease",          60,
  "Heart Diease",           58,
)
knitr::kable(outbreak_data)
```


# Standardization by string distance

The most straight forward way to standardize a character vector is to use string distance. String distance is a measure of how different two strings are. The method I tend to use is [Jaroâ€“Winkler](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance) distance. The distance returned is normalized so that a score of 0 represents an exact match between two strings and a 1 maximal difference. This kind of operation involves so called `fuzzy` logic which, unlike boolean definitions of true and false, handles the case of partial matches. In R, the main package for calculating string distance is the [stringdist](https://github.com/markvanderloo/stringdist) package. For this use case we need to come up with an algorith that will sequentially move down a vector, finding the best match within a given string distance and adopting it as the new standard. Graphically the algorithm should essentially do the following.

![Harmonize vector using stringdist](/assets/img/fuzzy_harmonize_stringdist.png)

To walk down the vector we could either use for loops or some kind of recursive function. Fortunately there is a function specifically for this kind of recursion in the purrr package, [reduce](https://blog.zhaw.ch/datascience/r-reduce-applys-lesser-known-brother/)! In order to use either approach we first need to define the function we want to apply between each element and the vector. I'm going to call the function `fuzzy_match` though that name may already be in use elsewhere.

```{r}
# This is pretty cool!
fuzzy_match <- function(vector, element, max_dist = 0.1) {
  
  # Return the closest matching element in the vector 
  # Add in a few guards against failure to match
  key <- vector[vector != element]
  dist <- stringdist::stringdist(element, key, method = "jw")
  if(!is.null(element)) {
    if(!is.na(element)) {
      if(length(dist) > 0 & any(!is.na(dist))) {
        if(min(dist, na.rm = T) < max_dist) return(c(vector, key[which.min(dist)]))
      }
    }
  }

  # If no match within max_dist is found, return the original element
  return(c(vector, element))
}
  
```

Now we just need to invoke it using the reduce function as so:

``` {r}
# I'm pretty proud of this one!
outbreak_data <- outbreak_data |> 
  mutate(standardized_disease_name = reduce(disease_name, fuzzy_match, max_dist = 0.1))
```

which produces the following:

```{r, echo = F}
knitr::kable(outbreak_data)
```

Notice how not everything was standardized? That's because we chose a relatively stringent maximum disease distance. If we instead set `max_dist = 0.3` we get the following:

```{r, echo = F}
outbreak_data |> 
  mutate(standardized_disease_name = reduce(disease_name, fuzzy_match, max_dist = 0.3)) |> 
  knitr::kable()
```

The weakness of this approach is that it is not always clear what the best maximum distance to use is. If you set the maximum distance high enough you might be able to match difficult cases, such as `COVID-19 and sars-covid-19` but you run the risk of accidentally changing something too far, like matching `Heart Disease to Malaria`. It's is not always clear where that border should be drawn.


## Clustering

Another way to do essentially the same thing is through hierarchical clustering.

3.  Natural Language Processing (NLP)
