---
layout : post
title : PRML_study_chap_1_basic
subtitle : 
date : 2021-09-10
#categories:
tags : [datascience, PRML1]
toc_sticky : true
use_math : true
comments: true
---



### PRML introduction

training phase, learning phase, training data, test set, generalization, .....

generalization : 가지고 있는 것은 표본 밖에 없기 때문에, 학습된 모델이 얼마나 general하게 사용될 수 있는지 그 성능을 파악하는 것은 중요하다. 

feature extraction : 학습 이전에 feature를 미리 확인하고, 학습에 적합한(필요한) feature를 추출해낸 후, 학습에 용이한 형태로 변환하는 것. 핵심적인 feature를 유지한 채로 computing 속도 향상을 위해

supervised learning : training data가 input, output(target) vector 모두 가지는 경우

unsupervised learning : training data가 input vector만 가지는 경우 e.g. clustering, density estimation, project high-dimensional space into lower dimension for the purpose of visualization.

reinforcement learning : 보상을 최대화 하기 위한 적절한 행동을 찾아내는 문제, 그렇게 하도록 학습시키는 문제, 학습에서 최적 output을 미리 받는 것은 아니고 trial and error를 통해 이를 찾아내는데 관심을 가지고 있다.

#### Example : Polynomial Curving Fitting

polynomial curving fitting에서는 polynomial의 차수를 결정하는 것이 중요하다. 

아래 그림이 이를 잘 설명하고 있다. 

<그림 1>

차수가 커질수록 추정하는 polynomial이 flexible 하며 oscillation을 잘 잡아낼 수 있다. 


<br>

polynomial 의 차수와 해당 계수 크기 사이의 관계 또한 존재한다. 아래 그림을 보자

<그림 2>

그림에서 알 수 있듯, M의 크기가 커지면 즉 polynomial의 차원이 클수록 추정 계수값이 커진다는 것을 알 수 있다.(왜? 차수가 커지면서 더욱 복잡한 모형을 표현할 수 있게 되고, 데이터 하나하나에 곡선이 맞춰지다보니 곡선 자체가 복잡해지고 계수값은 매우 커진다). 또한 모델이 복잡하다보니 range 양 끝 부분은 더더욱 oscillation이 더욱 심해진다. 

<br>

data size 와 fitting : data size 가 클수록 overfitting 문제를 해결하는 데에 용이하다. 또한 data size가 클수록 복잡한 모델의 표현이 가능해진다. LSE 혹은 MLE는 모델을 먼저 설정하기 때문에 오버피팅의 문제를 피할수 없고, 모델 내의 파라미터 개수(모델 복잡성)의 5배 혹은 10배 이상의 데이터 개수가 있어야 학습이 용이하다. 

그런데 데이터의 개수에 따라서 모델의 파라미터를 한정짓는다는 것은 일반적인 관점에서 볼때 한계가 있다. 우리는 모델링을 할 때 우선적으로 문제를 정의하고 그에 맞는 모델을 설정해야하기 때문이다. 베이지안의 관점은 좀 다르다. 데이터의 사이즈에 따라서 적절한 파라미터 개수를 조절한다. 

기존의 관점을 고수할 때, 데이터 사이즈가 작은 경우 복잡한 모델을 어떻게 만들어 낼 수 있을까?  =>> regularization(shrinkage/ ridge and lasso / weight decay in neural net) 


지금까지의 논의들(모델의 차수, 오버피팅, data size) 등은 결국 model complexity를 설명하기 위한 것이다. taking data -> model complexity 설정 -> train / validation set  (이 문장은 별 필요 없는 듯)




#### 1.2 Probability Theory

bayes' theorem , prior, posterior, independent, change of variable, CDF, Expectation, Covariance, bayesian, MLE, MCMC, Monte Carlo metod, variational Bayes..




1. change of variable

$$
p_y(y) = p_x(x)|\frac{dx}{dy}|
$$

2. MLE

    관찰된 데이터 셋의 확률을 가장 크게 만드는 모수 추정량

3. frequentist error bar : bootstrap

4. bayesian vs frequentist
    
    동전 던지기 결과 100번 모두 앞면이 나왔다면? frequentist 들은 나온 표본을 가지고 판정하는데 중점을 둔다. 따라서 앞면이 나올 확률를 나타내는 모수 p 에 대한 mle는 1. but bayesian 관점에서는 사전분포를 고려하기 때문에 이렇게 극단적인 값은 안나올 것.  







