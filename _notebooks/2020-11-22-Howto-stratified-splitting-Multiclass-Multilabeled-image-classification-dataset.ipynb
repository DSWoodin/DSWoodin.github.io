{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do stratified splitting of Multi-class Multi-labeled image classification datasets\n",
    "\n",
    "I saw a [post on StackOverflow](https://stackoverflow.com/questions/64838108/multi-labeled-image-classification-with-imbalanced-data-how-to-split-it) about how to do stratified sampling with multi-class, multi-label image data. Stratified sampling is imporant when you have extremely unbalanced machine learning datasets to ensure that each class is evenly distributed across your train/test/validation splits.\n",
    "\n",
    "Here's a handy visual for stratified sampling from Wikipedia:\n",
    "![Stratified Sampling example. Source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/f/fa/Stratified_sampling.PNG)\n",
    "\n",
    "I had some code which solves this exact problem, so I am making this available here as-is.\n",
    "\n",
    "The solution depends on `skmultilearn`'s `IterativeStratification` method. Unfortunately, `skmultilearn` is not very well maintained and I ran into a few sharp corners while coming up with this solution. I documented those sharp corners in the comments below.\n",
    "\n",
    "Also note that these function is used inside a Kedro pipeline, where I am using Kedro's PartitionDatasets. That's why the return values are sometimes packaged inside a dictionary.\n",
    "\n",
    "I am also making available my `pytest` suites at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "import ipytest\n",
    "import pytest\n",
    "from typeguard import typechecked\n",
    "from skmultilearn.model_selection.iterative_stratification import IterativeStratification\n",
    "\n",
    "# enables ipytest notebook magic\n",
    "ipytest.autoconfig()\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def stratify_shuffle_split_subsets(\n",
    "    full_dataset: Dict[str, Callable], output_partition_name: str, train_fraction: float\n",
    ") -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Stratify-shuffle-split the a multi-class multi-label dataset into\n",
    "    train/dev/validation sets.\n",
    "\n",
    "    Args:\n",
    "        full_dataset: the full supervised dataset. One column is the img uris, and the rest are binary labels.\n",
    "        output_partition_name: the name of the output partition\n",
    "        train_fraction: the fraction of data to reserve for the training dataset. The remaining data will be evenly\n",
    "            split into the dev and validation subsets.\n",
    "\n",
    "    Returns:\n",
    "        the supervised dataset, split into train/test/dev subsets. The train subset is not saved as a partitionedDataset\n",
    "        yet because we are doing label smoothing in the next processing step.\n",
    "    \"\"\"\n",
    "    if not 0 < train_fraction < 1:\n",
    "        raise ValueError(f\"Training fraction must be value between 0 and 1, got {train_fraction}\")\n",
    "\n",
    "    loader = full_dataset[output_partition_name]\n",
    "    full_dataset = loader()\n",
    "    original_cols = full_dataset.columns\n",
    "    # pandas documentation says to use .to_numpy() instead of .values for consistency\n",
    "    img_urls = full_dataset[\"img_url\"].to_numpy()\n",
    "\n",
    "    # sanity check: no duplicate labels\n",
    "    if not len(img_urls) == len(set(img_urls)):\n",
    "        raise ValueError(\"Duplicate image keys detected.\")\n",
    "\n",
    "    labels = full_dataset.drop(columns=[\"img_url\"]).to_numpy().astype(int)\n",
    "    # NOTE generators are replicated across workers. do stratified shuffle split beforehand\n",
    "    logger.info(\"stratifying dataset iteratively. this may take a while.\")\n",
    "    # NOTE: splits >2 broken; https://github.com/scikit-multilearn/scikit-multilearn/issues/209\n",
    "    # so, do 2 rounds of iterative splitting\n",
    "    stratifier = IterativeStratification(\n",
    "        n_splits=2, order=2, sample_distribution_per_fold=[1.0 - train_fraction, train_fraction],\n",
    "    )\n",
    "    # this class is a generator that produces k-folds. we just want to iterate it once to make a single static split\n",
    "    # NOTE: needs to be computed on hard labels.\n",
    "    train_indexes, everything_else_indexes = next(stratifier.split(X=img_urls, y=labels))\n",
    "\n",
    "    num_overlapping_samples = len(set(train_indexes).intersection(set(everything_else_indexes)))\n",
    "    if num_overlapping_samples != 0:\n",
    "        raise ValueError(f\"First splitting failed, {num_overlapping_samples} overlapping samples detected\")\n",
    "\n",
    "    # s3url array shape (N_samp,)\n",
    "    x_train, x_else = img_urls[train_indexes], img_urls[everything_else_indexes]\n",
    "    # labels array shape (N_samp, n_classes)\n",
    "    Y_train, Y_else = labels[train_indexes, :], labels[everything_else_indexes, :]\n",
    "\n",
    "    # now, split the \"else\" data evenly into dev/val splits\n",
    "    stratifier = IterativeStratification(n_splits=2, order=2)  # splits evenly by default\n",
    "    dev_indexes, validation_indexes = next(stratifier.split(x_else, Y_else))\n",
    "\n",
    "    num_overlapping_samples = len(set(dev_indexes).intersection(set(validation_indexes)))\n",
    "    if num_overlapping_samples != 0:\n",
    "        raise ValueError(f\"Second splitting failed, {num_overlapping_samples} overlapping samples detected\")\n",
    "\n",
    "    x_dev, x_val = (x_else[dev_indexes], x_else[validation_indexes])\n",
    "    Y_dev, Y_val = (Y_else[dev_indexes, :], Y_else[validation_indexes, :])\n",
    "\n",
    "    for subset_name, frac, encodings_collection in [\n",
    "        (\"train\", train_fraction, Y_train),\n",
    "        (\"dev\", (1.0 - train_fraction) / 2, Y_dev),\n",
    "        (\"val\", (1.0 - train_fraction) / 2, Y_val),\n",
    "    ]:\n",
    "        # column-wise sum. sum(counts) > n_samples due to imgs with >1 class\n",
    "        count_values = np.sum(encodings_collection, axis=0)\n",
    "        # skip first col, which is the image key, not a class ID\n",
    "        counts = {class_id: count_val for class_id, count_val in zip(full_dataset.columns[1:], count_values)}\n",
    "        logger.info(f\" {subset_name} subset ({frac * 100:.1f}%) encodings counts after stratification: {counts}\")\n",
    "\n",
    "    # combine (x,y) data into dataframes\n",
    "    train_subset = pd.DataFrame(Y_train)\n",
    "    train_subset.insert(0, \"img_url\", pd.Series(x_train))\n",
    "    train_subset.columns = original_cols\n",
    "\n",
    "    dev_subset = pd.DataFrame(Y_dev)\n",
    "    dev_subset.insert(0, \"img_url\", pd.Series(x_dev))\n",
    "    dev_subset.columns = original_cols\n",
    "\n",
    "    val_subset = pd.DataFrame(Y_val)\n",
    "    val_subset.insert(0, \"img_url\", pd.Series(x_val))\n",
    "    val_subset.columns = original_cols\n",
    "\n",
    "    # TODO create Great Expectations suite for this node instead\n",
    "    assert \"img_url\" in dev_subset.columns\n",
    "    assert \"img_url\" in val_subset.columns\n",
    "    assert \"img_url\" in train_subset.columns\n",
    "\n",
    "    return (\n",
    "        train_subset,\n",
    "        {output_partition_name: dev_subset},\n",
    "        {output_partition_name: val_subset},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing that it all works\n",
    "\n",
    "Below are my pytests and their results. Note, that the fixtures mimic Kedro PartitionDatasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def unsplit_supervised_learning_dataset_partition_name():\n",
    "    \"\"\"PartitionName to simulate Kedro PartitionDataset\"\"\"\n",
    "    return \"pytest_unsplit_supervised_learning_dataset_partition_name\"\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def unsplit_supervised_learning_dataset(unsplit_supervised_learning_dataset_partition_name: str) -> Dict[str, Callable]:\n",
    "    \"\"\"\n",
    "    Generates a multi-class multilabel dataset.\n",
    "\n",
    "    Args:\n",
    "        unsplit_supervised_learning_dataset_partition_name:\n",
    "\n",
    "    Returns:\n",
    "        a multi-class multi-label df, packed in a lambda to mimic Kedro PartitionDataset\n",
    "    \"\"\"\n",
    "    df_rows = []\n",
    "\n",
    "    n_total = 60\n",
    "    n_created = 0\n",
    "    n_class1 = n_total // 3\n",
    "    n_class2 = n_total // 3\n",
    "    n_class2_plus_3 = n_total // 3\n",
    "\n",
    "    for _ in range(n_class1):\n",
    "        df_rows.append({\"img_url\": f\"s3://test{n_created}\", \"class0\": 1, \"class1\": 0, \"class2\": 0})\n",
    "        n_created += 1\n",
    "\n",
    "    for _ in range(n_class2):\n",
    "        df_rows.append({\"img_url\": f\"s3://test{n_created}\", \"class0\": 0, \"class1\": 1, \"class2\": 0})\n",
    "        n_created += 1\n",
    "\n",
    "    for _ in range(n_class2_plus_3):\n",
    "        df_rows.append({\"img_url\": f\"s3://test{n_created}\", \"class0\": 0, \"class1\": 1, \"class2\": 1})\n",
    "        n_created += 1\n",
    "\n",
    "    df = pd.DataFrame(df_rows)\n",
    "    data_loader = lambda: df\n",
    "\n",
    "    return {unsplit_supervised_learning_dataset_partition_name: data_loader}\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"train_fraction\", [-1.0, 0, 1, 5])\n",
    "def test_train_fraction_range(\n",
    "    unsplit_supervised_learning_dataset, train_fraction, unsplit_supervised_learning_dataset_partition_name\n",
    "):\n",
    "    \"\"\"\n",
    "    Tests that train_fraction is validated between 0, 1.\n",
    "    \"\"\"\n",
    "    with pytest.raises(ValueError):\n",
    "        stratify_shuffle_split_subsets(\n",
    "            full_dataset=unsplit_supervised_learning_dataset,\n",
    "            output_partition_name=unsplit_supervised_learning_dataset_partition_name,\n",
    "            train_fraction=train_fraction,\n",
    "        )\n",
    "\n",
    "\n",
    "def test_duplicate_uris_detected(unsplit_supervised_learning_dataset_partition_name):\n",
    "    \"\"\"Tests whether duplicate image keys are properly detected\"\"\"\n",
    "    # setup\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            {\"img_url\": \"s3://duplicate1\", \"class0\": 1, \"class1\": 0, \"class2\": 0},\n",
    "            {\"img_url\": \"s3://duplicate1\", \"class0\": 1, \"class1\": 0, \"class2\": 0},\n",
    "            {\"img_url\": \"s3://duplicate1\", \"class0\": 1, \"class1\": 0, \"class2\": 0},\n",
    "            {\"img_url\": \"s3://duplicate2\", \"class0\": 0, \"class1\": 1, \"class2\": 0},\n",
    "            {\"img_url\": \"s3://duplicate2\", \"class0\": 0, \"class1\": 1, \"class2\": 0},\n",
    "        ]\n",
    "    )\n",
    "    data_loader = lambda: df\n",
    "    partition_dset = {unsplit_supervised_learning_dataset_partition_name: data_loader}\n",
    "\n",
    "    # test\n",
    "    with pytest.raises(ValueError):\n",
    "        stratify_shuffle_split_subsets(\n",
    "            full_dataset=partition_dset,\n",
    "            output_partition_name=unsplit_supervised_learning_dataset_partition_name,\n",
    "            train_fraction=0.6,\n",
    "        )\n",
    "\n",
    "        \n",
    "def test_col_names_same(unsplit_supervised_learning_dataset, unsplit_supervised_learning_dataset_partition_name):\n",
    "    \"\"\"\n",
    "    Tests that train_fraction is validated between 0, 1.\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    train_split: pd.DataFrame\n",
    "    dev_split: Dict[str, pd.DataFrame]\n",
    "    val_split: Dict[str, pd.DataFrame]\n",
    "    # set train fraction to 0.6 to make math easier later\n",
    "    train_fraction = 0.6\n",
    "    data_loader = unsplit_supervised_learning_dataset[unsplit_supervised_learning_dataset_partition_name]\n",
    "    original_df = data_loader()\n",
    "    original_df_cols = original_df.columns\n",
    "\n",
    "    # run\n",
    "    train_df, dev_split, val_split = stratify_shuffle_split_subsets(\n",
    "        full_dataset=unsplit_supervised_learning_dataset,\n",
    "        output_partition_name=unsplit_supervised_learning_dataset_partition_name,\n",
    "        train_fraction=train_fraction,\n",
    "    )\n",
    "\n",
    "    # test setup\n",
    "    dev_df, val_df = (\n",
    "        dev_split[unsplit_supervised_learning_dataset_partition_name],\n",
    "        val_split[unsplit_supervised_learning_dataset_partition_name],\n",
    "    )\n",
    "    train_df_cols, dev_df_cols, val_df_cols = train_df.columns, dev_df.columns, val_df.columns\n",
    "\n",
    "    # tests\n",
    "    assert all(train_df_cols == original_df_cols)\n",
    "    assert all(dev_df_cols == original_df_cols)\n",
    "    assert all(val_df_cols == original_df_cols)\n",
    "\n",
    "    \n",
    "def test_stratify_shuffle_split_subsets(\n",
    "    unsplit_supervised_learning_dataset, unsplit_supervised_learning_dataset_partition_name\n",
    "):\n",
    "    \"\"\"Tests whether a multi-class multi-label dataset gets properly stratify-shuffle-split.\"\"\"\n",
    "    # setup\n",
    "    train_split: pd.DataFrame\n",
    "    dev_split: Dict[str, pd.DataFrame]\n",
    "    val_split: Dict[str, pd.DataFrame]\n",
    "    # set train fraction to 0.6 to make math easier later\n",
    "    train_fraction = 0.6\n",
    "    ratio_train_to_val = train_fraction / ((1.0 - train_fraction) / 2)\n",
    "    data_loader = unsplit_supervised_learning_dataset[unsplit_supervised_learning_dataset_partition_name]\n",
    "    original_dset_len = len(data_loader())\n",
    "\n",
    "    # run\n",
    "    train_df, dev_split, val_split = stratify_shuffle_split_subsets(\n",
    "        full_dataset=unsplit_supervised_learning_dataset,\n",
    "        output_partition_name=unsplit_supervised_learning_dataset_partition_name,\n",
    "        train_fraction=train_fraction,\n",
    "    )\n",
    "\n",
    "    # test setup\n",
    "    dev_df, val_df = (\n",
    "        dev_split[unsplit_supervised_learning_dataset_partition_name],\n",
    "        val_split[unsplit_supervised_learning_dataset_partition_name],\n",
    "    )\n",
    "    train_keys, dev_keys, val_keys = train_df[\"img_url\"], dev_df[\"img_url\"], val_df[\"img_url\"]\n",
    "\n",
    "    uniq_train_keys, uniq_dev_keys, uniq_val_keys = set(train_keys), set(dev_keys), set(val_keys)\n",
    "\n",
    "    # Tests\n",
    "\n",
    "    ## test keys\n",
    "\n",
    "    # test no duplicate keys within subsets\n",
    "    assert len(train_keys) == len(uniq_train_keys)\n",
    "    assert len(val_keys) == len(uniq_val_keys)\n",
    "    assert len(dev_keys) == len(uniq_dev_keys)\n",
    "\n",
    "    # test that all subsets have mutually exclusive S3 URIs\n",
    "    assert len(uniq_train_keys.intersection(uniq_dev_keys)) == 0\n",
    "    assert len(uniq_train_keys.intersection(uniq_val_keys)) == 0\n",
    "    assert len(uniq_val_keys.intersection(uniq_dev_keys)) == 0\n",
    "\n",
    "    ## test ratios\n",
    "    # test that dev and val subsets get same number of samples\n",
    "    assert len(dev_df) == len(val_df)\n",
    "    # test that test set is 2x size of dev\n",
    "    assert len(train_df) / (len(train_df) + len(dev_df) + len(val_df)) == pytest.approx(train_fraction)\n",
    "    # test that all samples are used\n",
    "    assert len(train_df) + len(dev_df) + len(val_df) == original_dset_len\n",
    "\n",
    "    ## test distributions\n",
    "\n",
    "    ### class 0\n",
    "    # test whether dev/val set got same amount of class 0\n",
    "    assert len(dev_df[dev_df.class0 == 1]) == len(val_df[val_df.class0 == 1])\n",
    "    # test whether train set got 2x class0 as val set\n",
    "    assert len(train_df[train_df.class0 == 1]) / len(val_df[val_df.class0 == 1]) == pytest.approx(ratio_train_to_val)\n",
    "\n",
    "    ### class 1\n",
    "    # test whether dev/val set got same amount of class 1\n",
    "    assert len(dev_df[dev_df.class1 == 1]) == len(val_df[val_df.class1 == 1])\n",
    "    # test whether train set got 2x class1 as val set\n",
    "    assert len(train_df[train_df.class1 == 1]) / len(val_df[val_df.class1 == 1]) == pytest.approx(ratio_train_to_val)\n",
    "\n",
    "    ### class 2\n",
    "    # test whether dev/val set got same amount of class 2\n",
    "    assert len(dev_df[dev_df.class2 == 1]) == len(val_df[val_df.class2 == 1])\n",
    "    # test whether train set got 2x class1 as val set\n",
    "    assert len(train_df[train_df.class2 == 1]) / len(val_df[val_df.class2 == 1]) == pytest.approx(ratio_train_to_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....2020-11-22 17:19:46,228 - __main__ - INFO - stratifying dataset iteratively. this may take a while.\n",
      "2020-11-22 17:19:46,239 - __main__ - INFO -  train subset (60.0%) encodings counts after stratification: {'class0': 12, 'class1': 24, 'class2': 12}\n",
      "2020-11-22 17:19:46,251 - __main__ - INFO -  dev subset (20.0%) encodings counts after stratification: {'class0': 4, 'class1': 8, 'class2': 4}\n",
      "2020-11-22 17:19:46,253 - __main__ - INFO -  val subset (20.0%) encodings counts after stratification: {'class0': 4, 'class1': 8, 'class2': 4}\n",
      ".2020-11-22 17:19:46,271 - __main__ - INFO - stratifying dataset iteratively. this may take a while.\n",
      "2020-11-22 17:19:46,280 - __main__ - INFO -  train subset (60.0%) encodings counts after stratification: {'class0': 12, 'class1': 24, 'class2': 12}\n",
      "2020-11-22 17:19:46,283 - __main__ - INFO -  dev subset (20.0%) encodings counts after stratification: {'class0': 4, 'class1': 8, 'class2': 4}\n",
      "2020-11-22 17:19:46,287 - __main__ - INFO -  val subset (20.0%) encodings counts after stratification: {'class0': 4, 'class1': 8, 'class2': 4}\n",
      ".                                                                                         [100%]\n",
      "========================================== warnings summary ===========================================\n",
      "/home/richard/src/DENDRA/seeweed/venv/lib/python3.7/site-packages/_pytest/config/__init__.py:1040\n",
      "  /home/richard/src/DENDRA/seeweed/venv/lib/python3.7/site-packages/_pytest/config/__init__.py:1040: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: typeguard\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "7 passed, 1 warning in 0.15s\n"
     ]
    }
   ],
   "source": [
    "ipytest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "If you found this helpful, feel free to upvote my answer for the [original question](https://stackoverflow.com/questions/64838108/multi-labeled-image-classification-with-imbalanced-data-how-to-split-it). Also, if you have suggestions please let me know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SeeWeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
