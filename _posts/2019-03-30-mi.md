
---
layout: post
title: CREDIT SCORECARD 02 
subtitle: just basic!
---


# CREDIT SCORECARD 02 

Nguyễn Quang Minh

Bài viết thứ 2 nằm trong series về **Credit Scorecard** trong các tổ chức tín dụng. Trong bài này chúng ta cùng tìm hiểu cách ứng dụng các mô hình Machine trong chấm điểm tín dụng cũng như tìm ra mô hình tối ưu nhất. 

**Credit Scorecard** với mục tiêu là chấm điểm và phân loại khách hàng, cho phép đưa ra quyết định về chính sách can thiệp nhằm tối ưu hóa hiệu quả kinh doanh.

Thí dụ minh họa là một dữ liệu về thông tin cơ bản cũng như lịch sử hành vi của khách hàng thuộc nhóm trễ hạn nợ 2 tháng (B2_BOM).

Bài toán được giải quyết dựa trên dữ liệu của 10000 khách hàng, bao gồm nhiều thông tin, các thông tin đã được biến đổi cũng như thay đổi tên trường để đảm bảo lý do bảo mật.

Biến output : **label**
- label = 1: khách hàng không thanh toán 
- label = 0: khách hàng thanh toán


Mục tiêu gồm : 
1. Các phương pháp chuyển dạng, chuẩn hóa dữ liệu
2. Giới thiệu các thuật toán machine learning cơ bản 
3. Turning model sử dụng GridSearchCV và tìm ra mô hình tối ưu

_______________________________

Các thư viện cần dùng:


```python
import numpy as np
import pandas as pd

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier


import matplotlib.pyplot as plt
%matplotlib inline

import warnings
warnings.filterwarnings('ignore')
```


```python
df = pd.read_csv('credit_data.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>field0</th>
      <th>field1</th>
      <th>field2</th>
      <th>field3</th>
      <th>field4</th>
      <th>field5</th>
      <th>field6</th>
      <th>field7</th>
      <th>field8</th>
      <th>field9</th>
      <th>...</th>
      <th>field32</th>
      <th>field33</th>
      <th>field34</th>
      <th>field35</th>
      <th>field36</th>
      <th>field37</th>
      <th>field38</th>
      <th>field39</th>
      <th>field40</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.513677</td>
      <td>0.958333</td>
      <td>field2_v0</td>
      <td>field3_v0</td>
      <td>0.666667</td>
      <td>0.0</td>
      <td>0.557377</td>
      <td>0.553003</td>
      <td>0.941884</td>
      <td>field9_v0</td>
      <td>...</td>
      <td>0.333333</td>
      <td>0.166667</td>
      <td>0.285714</td>
      <td>field35_v0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>field38_v0</td>
      <td>0.882544</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.426661</td>
      <td>0.541667</td>
      <td>field2_v1</td>
      <td>field3_v0</td>
      <td>0.666667</td>
      <td>0.0</td>
      <td>0.655738</td>
      <td>0.407874</td>
      <td>0.608551</td>
      <td>field9_v0</td>
      <td>...</td>
      <td>0.166667</td>
      <td>0.000000</td>
      <td>0.142857</td>
      <td>field35_v0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>field38_v1</td>
      <td>0.977390</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.031936</td>
      <td>1.000000</td>
      <td>field2_v2</td>
      <td>field3_v1</td>
      <td>0.666667</td>
      <td>0.0</td>
      <td>0.868852</td>
      <td>0.247613</td>
      <td>0.536232</td>
      <td>field9_v0</td>
      <td>...</td>
      <td>0.500000</td>
      <td>0.333333</td>
      <td>0.285714</td>
      <td>field35_v0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>field38_v2</td>
      <td>0.115773</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.210625</td>
      <td>0.291667</td>
      <td>field2_v1</td>
      <td>field3_v2</td>
      <td>0.500000</td>
      <td>0.0</td>
      <td>0.524590</td>
      <td>0.262745</td>
      <td>1.000000</td>
      <td>field9_v1</td>
      <td>...</td>
      <td>0.166667</td>
      <td>0.166667</td>
      <td>0.285714</td>
      <td>field35_v0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>field38_v3</td>
      <td>0.723744</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.291021</td>
      <td>0.833333</td>
      <td>field2_v1</td>
      <td>field3_v2</td>
      <td>0.500000</td>
      <td>1.0</td>
      <td>0.475410</td>
      <td>0.262745</td>
      <td>0.859420</td>
      <td>field9_v0</td>
      <td>...</td>
      <td>0.166667</td>
      <td>0.000000</td>
      <td>0.285714</td>
      <td>field35_v0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>field38_v4</td>
      <td>1.000000</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 42 columns</p>
</div>




```python
df.hist(figsize=(16,13))
plt.show()
```


![png](output_4_0.png)


## Bước 1: TRANFORM DATA

Do dữ liệu đã được clean nên chúng ta đến thẳng bước tranform data để chuẩn bị training model machine learning

### OneHotEncoder

Kỹ thuật onehot (hay dummy) là kỹ thuật chuyển dạng dữ liệu từ dạng **chuỗi** sang dạng bool mà vẫn giữ nguyên được giá trị thông tin của dữ liệu. Với phương thức là với mỗi giá trị trong trường dữ liệu cũ ta tạo một trường dữ liệu mới thể hiện rằng dữ liệu tại đó có bằng giá trị đó không. 

Chúng ta sẽ thấy rõ nó hoạt động thế nào sau bước dưới đây:


```python
# chọn các trường dữ liệu chuỗi 
object_cols = list(df.dtypes[df.dtypes == 'object'].index)
df[object_cols].head(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>field2</th>
      <th>field3</th>
      <th>field9</th>
      <th>field10</th>
      <th>field13</th>
      <th>field35</th>
      <th>field38</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>field2_v0</td>
      <td>field3_v0</td>
      <td>field9_v0</td>
      <td>field10_v0</td>
      <td>field13_v0</td>
      <td>field35_v0</td>
      <td>field38_v0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>field2_v1</td>
      <td>field3_v0</td>
      <td>field9_v0</td>
      <td>field10_v1</td>
      <td>field13_v1</td>
      <td>field35_v0</td>
      <td>field38_v1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>field2_v2</td>
      <td>field3_v1</td>
      <td>field9_v0</td>
      <td>field10_v2</td>
      <td>field13_v2</td>
      <td>field35_v0</td>
      <td>field38_v2</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn.preprocessing import OneHotEncoder

onehot = OneHotEncoder(handle_unknown='ignore')
onehot_data = pd.DataFrame(onehot.fit_transform(df[object_cols]).toarray(),
                           columns=onehot.get_feature_names(object_cols)) # đặt columns names

# data sau khi OneHotEncoder
onehot_data.loc[:2,['field2_field2_v0','field2_field2_v1', 'field2_field2_v2']]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>field2_field2_v0</th>
      <th>field2_field2_v1</th>
      <th>field2_field2_v2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# tổng hợp dữ liệu đã được onehot 
data = pd.concat([onehot_data, df.drop(object_cols,axis=1)],axis=1)
data.head(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>field2_field2_v0</th>
      <th>field2_field2_v1</th>
      <th>field2_field2_v2</th>
      <th>field3_field3_v0</th>
      <th>field3_field3_v1</th>
      <th>field3_field3_v2</th>
      <th>field3_field3_v3</th>
      <th>field3_field3_v4</th>
      <th>field9_field9_v0</th>
      <th>field9_field9_v1</th>
      <th>...</th>
      <th>field30</th>
      <th>field31</th>
      <th>field32</th>
      <th>field33</th>
      <th>field34</th>
      <th>field36</th>
      <th>field37</th>
      <th>field39</th>
      <th>field40</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.775</td>
      <td>0.333333</td>
      <td>0.166667</td>
      <td>0.285714</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.882544</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>1.000000</td>
      <td>0.000</td>
      <td>0.166667</td>
      <td>0.000000</td>
      <td>0.142857</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.977390</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.010101</td>
      <td>0.575</td>
      <td>0.500000</td>
      <td>0.333333</td>
      <td>0.285714</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.115773</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 158 columns</p>
</div>



### MinMaxScaler

Là kỹ thuật chuẩn hóa dữ liệu phổ biến cho các dữ liệu **Nunberic**. Với cơ chế là đưa dữ liệu về 1 thang đo mới 
vd: *MinMaxScaler(feature_range=(-1,1))*
   - -1 : giá trị nhỏ nhất 
   -   1 : giá trị lớn nhất 

**Tại sao chuẩn hóa dữ liệu lại cẩn thiết ?** 

Vì khi tính toán với nhiều trường dữ liệu khác nhau thì sẽ có các phép toán kiểu như : **age** + **loan** 
Giả sử khách hàng 45 tuổi và vay 50 triệu thì việc lấy số 45 tuổi + 50 triệu có sự bất đồng lớn về thang đo tính toán

Do vậy ta cần đưa **age** và **loan** về cùng thang đo (-1,1) để đạt được hiệu quả cao khi traing model

*Tuy dữ liệu đã được chuẩn hóa nhưng ta vẫn thực hiện lại bước này để demo*


```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(-1,1))

# chọn các trường dữ liệu Nunberic trừ 'label'
df_num = df.drop(['label'], axis=1)._get_numeric_data()

# scale and transform data 
scale_data = scaler.fit_transform(df_num )
scale_data = pd.DataFrame(scale_data, columns=df_num.columns)

# tổng hợp dữ liệu đã được scale 
for col in scale_data.columns:
    data[col] = scale_data[col]
data.head(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>field2_field2_v0</th>
      <th>field2_field2_v1</th>
      <th>field2_field2_v2</th>
      <th>field3_field3_v0</th>
      <th>field3_field3_v1</th>
      <th>field3_field3_v2</th>
      <th>field3_field3_v3</th>
      <th>field3_field3_v4</th>
      <th>field9_field9_v0</th>
      <th>field9_field9_v1</th>
      <th>...</th>
      <th>field30</th>
      <th>field31</th>
      <th>field32</th>
      <th>field33</th>
      <th>field34</th>
      <th>field36</th>
      <th>field37</th>
      <th>field39</th>
      <th>field40</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>-1.000000</td>
      <td>0.55</td>
      <td>-0.333333</td>
      <td>-0.666667</td>
      <td>-0.428571</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>0.765088</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>1.000000</td>
      <td>-1.00</td>
      <td>-0.666667</td>
      <td>-1.000000</td>
      <td>-0.714286</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>0.954780</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>-0.979798</td>
      <td>0.15</td>
      <td>0.000000</td>
      <td>-0.333333</td>
      <td>-0.428571</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>-0.768454</td>
      <td>-1.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 158 columns</p>
</div>



## Bước 2 : TRAINING MODEL

Sau khi dữ liệu đã sạch sẽ thơm tho thì chún ta có thể đem đi xào nấu thỏa thích với đủ thể loại thuật toán machine learning

Việc đầu tiên chúng ta cần làm là chia tệp dữ liệu thành các bộ traing_set và test_set:


```python
from sklearn.model_selection import train_test_split

X = data.drop(['label'], axis=1)
y = data['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.3,
                                                    random_state = 15, 
                                                    stratify = y)

```

## Define models and gridsearchs

*Chỉ có 1 cách để biết được mô hình nào là tốt nhất là thử nghiệm thật nhiều loại mô hình.*

Nhưng lại có một vấn để như sau: 

Giả sử ta muốn tìm mô hình tốt hơn giữ mô hình Logistic và mô hình KNN, vậy đầu tiên ta phải tìm được mô hình Logistic tốt nhất và mô hình KNN tốt nhất, sau đó mới so sánh chúng với nhau để tìm ra cái tốt hơn trong những cái tốt.

Nghe hơi rối não nhưng ở đây chúng ta sẽ đề cạp đến 2 khái niệm để giải thích vấn đề này: 

**Parameter** vs **Hyper Parameter**:

- **Parameter** : Tham số trong các phương trình tối ưu của thuật toán , 
        vd: khi chúng ta xét hàm tuyến tính: f(x) = a*x + b thì a và b chính là các Parameter 
        => chúng ta cần tìm các parameter để mô hình fit với tập dữ liệu
- **Hyper Parameter**: Siêu tham số trong các mô hình machine learning
        vd: khi chúng ta định nghĩa mô hình RandomForestClassifier(max_features='sqrt', criterion='gini')
        Thì max_features và criterion là các hyper parameter của mô hình là các rule, thuật toán cụ thể mô hình sẽ sử dụng
        => chúng ta cần tìm các hyper parameter để tìm ra bộ setup tốt nhất cho mô hình machine learning


```python
classifiers = [
    [GridSearchCV(estimator=KNeighborsClassifier(n_jobs=-1), param_grid={
        'n_neighbors': [5, 7, 9, 11, 15, 20]
    }, n_jobs=-1), "KNN"],
    [GridSearchCV(estimator=SVC(), param_grid={
        'kernel': ['linear', 'rbf'],
        'C': [0.001, 0.01, 0.1, 1, 10],
        'gamma': [0.001, 0.01, 0.1, 1, 10]
    }, n_jobs=-1), "SVM"],
    [GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1), param_grid={
        'n_estimators': [200, 500],
        'max_features': ['auto', 'sqrt', 'log2'],
        'max_depth': [5, 7, 8, 12],
        'criterion': ['gini', 'entropy']
    }, n_jobs=-1), "RF"],
    [GridSearchCV(estimator=MLPClassifier(), param_grid={
        'solver': ['lbfgs'],
        'max_iter': [1400, 1500, 1600, 1700, 1800, 1900],
        'alpha': 10.0 ** -np.arange(1, 10),
        'hidden_layer_sizes': np.arange(10, 15),
        'random_state': [0, 1, 2]
    }, n_jobs=-1), "MLP"],
    [GridSearchCV(estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier()), param_grid={
        'n_estimators': (1, 2, 5, 10),
        'base_estimator__max_depth': (5, 8, 12),
        'algorithm': ('SAMME', 'SAMME.R')
    }, n_jobs=-1), "Ada"]
]
```


```python

```
